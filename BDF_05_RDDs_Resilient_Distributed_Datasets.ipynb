{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jalorenzo/SparkNotebook25-26/blob/main/BDF_05_RDDs_Resilient_Distributed_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvy135v4KPHL"
      },
      "source": [
        "#00 - Configuration of Apache Spark on Collaboratory\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iuen2fmUKSCQ"
      },
      "source": [
        "###Installing Java, Spark, and Findspark\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This code installs Apache Spark 2.2.1, Java 8, and [Findspark](https://github.com/minrk/findspark), a library that makes it easy for Python to find Spark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsAfQ0CrgnWf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"SPARK_VERSION\"] = \"spark-4.1.1\"\n",
        "!apt-get update\n",
        "!apt-get install openjdk-17-jdk-headless\n",
        "!wget https://archive.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!echo $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!rm $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFKUhFm4KZgM"
      },
      "source": [
        "### Set Environment Variables\n",
        "Set the locations where Spark and Java are installed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiOoj3rUgnVx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark/\"\n",
        "os.environ[\"DRIVE_DATA\"] = \"/content/gdrive/My Drive/Enseignement/2025-2026/ING3/HPDA/BigDataFrameworks/data/\"\n",
        "\n",
        "!rm /content/spark\n",
        "!ln -s /content/$SPARK_VERSION-bin-hadoop3 /content/spark\n",
        "!export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
        "!echo $SPARK_HOME\n",
        "!env |grep  \"DRIVE_DATA\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5zSFkyOKewE"
      },
      "source": [
        "### Start a SparkSession\n",
        "This will start a local Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoTPRSa-KK5C"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Example: shows the PySpark version\n",
        "print(\"PySpark version {0}\".format(sc.version))\n",
        "\n",
        "# Example: parallelise an array and show the 2 first elements\n",
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhW9r4HgKK3q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# We create a SparkSession object (or we retrieve it if it is already created)\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"My application\") \\\n",
        ".config(\"spark.some.config.option\", \"some-value\") \\\n",
        ".master(\"local[4]\") \\\n",
        ".getOrCreate()\n",
        "# We get the SparkContext\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mLhZ2iMKK1W"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqCEbDANgCWJ"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "# 05 - RDDs: Resilient Distributed Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY_P1QWJgPoU"
      },
      "source": [
        "\n",
        "-   Immutable distributed collection of objects that can be manipulated in parallel.\n",
        "    - Most basic data type in Spark\n",
        "\n",
        "-   A Spark program operates on RDDs:\n",
        "\n",
        "    -   RDDs creation\n",
        "\n",
        "    -   RDDs **transformation** (map, filter, etc.)\n",
        "\n",
        "    -   **Actions** on RDDs to obtain results\n",
        "    \n",
        "    Similar to DataFrames, but they provide a more precise control on the partitioning and data distribution\n",
        "\n",
        "-   Spark automatically distributes the data and parallelises the operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybw4YkQ7g_NF"
      },
      "source": [
        "## RDDs creation\n",
        "\n",
        "Two ways:\n",
        "\n",
        "1 -   By distributing a collection of objects (e.g., a list or set) in their driver program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmNv36fXho6I"
      },
      "outputs": [],
      "source": [
        "rdd1 = sc.parallelize([1,2,3,4,5,6,7,8])\n",
        "print(rdd1.collect())\n",
        "\n",
        "import numpy as np\n",
        "rdd2=sc.parallelize(np.array(range(100)))\n",
        "print(rdd2.collect())\n",
        "\n",
        "# RDDs accept lists of different types\n",
        "rdd5 = sc.parallelize([1,2,\"three\",4])\n",
        "print(rdd5.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6Ue7W8XiwBq"
      },
      "source": [
        "2 -   By loading an external dataset from an input such as a file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8vObfpHiu1r"
      },
      "outputs": [],
      "source": [
        "quijoteRDD = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"quijote.txt\")\n",
        "\n",
        "print(quijoteRDD.take(1000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOiqOaBUjNee"
      },
      "source": [
        "## Partitions\n",
        "\n",
        "Spark splits each RDD into multiple partitions, which may be computed on different nodes of the cluster\n",
        "\n",
        "-   The default number of partitions is function of the size of the cluster or the number of file blocks (for example, HDFS blocks)\n",
        "\n",
        "-   A different number of partitions can be defined when the RDD is created\n",
        "\n",
        "-  They can also be modified once created  (`repartition` or `coalesce`)\n",
        "\n",
        "-  The method `glom` shows how the partitions were created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-rwTuOnjl2A"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([1,2,3,4], 2)\n",
        "print(rdd.glom().collect())\n",
        "print(rdd.getNumPartitions())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XcddAzKj2il"
      },
      "source": [
        "## RDDs and DataFrames\n",
        "\n",
        "A DataFrame has an RDD underneath, that can be accessed in a simple way"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFeEiX37kDu2"
      },
      "outputs": [],
      "source": [
        "dfFlightData2015 = (spark\n",
        "    .read\n",
        "    .option(\"inferSchema\", \"true\")\n",
        "    .option(\"header\", \"true\")\n",
        "    .csv(os.environ[\"DRIVE_DATA\"] + \"2015-summary.csv\"))\n",
        "\n",
        "rddFlightData2015 = dfFlightData2015.rdd\n",
        "\n",
        "rddFlightData2015.take(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A844PtpGnteE"
      },
      "source": [
        "We can create a DataFrame from an RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG5G9L1rnkU9"
      },
      "outputs": [],
      "source": [
        "dfNew = spark.createDataFrame(rddFlightData2015, dfFlightData2015.schema)\n",
        "dfNew.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sykqj4CJo7CF"
      },
      "source": [
        "## Transformations and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKFWSW2ukoVT"
      },
      "source": [
        "### Transformations\n",
        "\n",
        "Operations on RDDs that return a new RDD\n",
        "\n",
        "-   Computed in a *lazy* way.\n",
        "\n",
        "-   They typically execute a function (anonymous or not) on each of the original RDD elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3FcbFS0pRlH"
      },
      "outputs": [],
      "source": [
        "quijs = quijoteRDD.filter(lambda l: \"Quijote\" in l)\n",
        "sanchs = quijoteRDD.filter(lambda l: \"Sancho\" in l)\n",
        "quijssancs = quijs.intersection(sanchs)\n",
        "print(quijssancs.take(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou0P7NxAplRd"
      },
      "source": [
        "### Actions\n",
        "\n",
        "Obtain output data from RDDs\n",
        "\n",
        "-   Return values to the driver or to the storage system\n",
        "\n",
        "-   Force the pending transformations to be applied"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfnqsVlqpsPw"
      },
      "outputs": [],
      "source": [
        "nqs = quijssancs.count()\n",
        "print(\"Lines with Quijote and Sancho {0}\".format(nqs))\n",
        "for l in quijssancs.takeSample(False,10):\n",
        "    print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfy4p9DPqBZC"
      },
      "source": [
        "## Common Transformations and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC8przcxqEtS"
      },
      "source": [
        "###Element-wise transformations\n",
        "\n",
        "\n",
        "Generate a new RDD from a given one by applying a function to each of the elements of the original RDD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi3s7Dupq-9k"
      },
      "source": [
        "-   `filter(func)` filters elements from an RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT8G89jErWYU"
      },
      "outputs": [],
      "source": [
        "#As seen before\n",
        "quijsRDD = quijoteRDD.filter(lambda l: \"Quijote\" in l)\n",
        "sanchsRDD = quijoteRDD.filter(lambda l: \"Sancho\" in l)\n",
        "quijssancsRDD = quijsRDD.intersection(sanchsRDD)\n",
        "quijssancsRDD.cache()\n",
        "print(\"Lines with Quijote and Sancho {0}\".format(quijssancsRDD.count()))\n",
        "for l in quijssancsRDD.takeSample(False,10):\n",
        "    print(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J01jDtPEIWer"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import unittest\n",
        "# Extract the positive values from a range of numbers\n",
        "\n",
        "#from test_helpers import Test\n",
        "rdd = sc.parallelize(range(-5,5))          # Range [-5, 5)\n",
        "print(rdd.collect())\n",
        "filtered_rdd = rdd.filter(lambda x: x >= 0)   # Returns only the positive values\n",
        "\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_positive(self):\n",
        "        self.assertEqual(filtered_rdd.collect(), [0, 1, 2, 3, 4])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-oTHAipIilD"
      },
      "source": [
        "-   `map(func)` applies a function to each element in an RDD, with the result of the function being the new value of each element in the resulting RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDYn4yH1Immz"
      },
      "outputs": [],
      "source": [
        "# Adds 1 to each element in the RDD\n",
        "# For each element, it obtains a tuple (x, x**2)\n",
        "def add1(x):\n",
        "    return(x+1)\n",
        "\n",
        "squared_rdd = (filtered_rdd\n",
        "               .map(add1)                 # Adds 1 to each element in the RDD\n",
        "               .map(lambda x: (x, x*x)))  # For each element, it obtains a tuple (x, x**2)\n",
        "\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_squared(self):\n",
        "        self.assertEqual(squared_rdd.collect(), [(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1L8UOO4IwLU"
      },
      "source": [
        "-   `flatMap(func)` similar to `map`, but “flattening” the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfH8BzCpIuxw"
      },
      "outputs": [],
      "source": [
        "squaredflat_rdd = (filtered_rdd\n",
        "                   .map(add1)\n",
        "                   .flatMap(lambda x: (x, x*x)))  # returns output as a list\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_squaredflat(self):\n",
        "        self.assertEqual(squaredflat_rdd.collect(), [1, 1, 2, 4, 3, 9, 4, 16, 5, 25])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juHuDhOmI8X5"
      },
      "source": [
        "-   `sample(withReplacement, fraction, seed=None)` returns a sample of the RDD\n",
        "    - `withReplacement` - if True, each element can show up several times in the sample\n",
        "    - `fraction` - expected sample size as a fraction of the RDD size\n",
        "        -  **without replacement**: probability of selecting an element, its value must be [0, 1]\n",
        "        -  **with replacement**: expected number of times that an element will be picked up, its value must be >= 0\n",
        "    - `seed` - seed for the random number generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvF12YTMJANM"
      },
      "outputs": [],
      "source": [
        "srdd1 = squaredflat_rdd.sample(False, 0.5)\n",
        "srdd2 = squaredflat_rdd.sample(True, 2)\n",
        "srdd3 = squaredflat_rdd.sample(False, 0.8, 14)\n",
        "print('s1={0}\\ns2={1}\\ns3={2}'.format(srdd1.collect(), srdd2.collect(), srdd3.collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBSl9tYbJFmK"
      },
      "source": [
        "-   `distinct()` returns a new RDD without duplicates\n",
        "    - The output order is not defined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-H6HYvZJVYb"
      },
      "outputs": [],
      "source": [
        "distinct_rdd = squaredflat_rdd.distinct()\n",
        "print(distinct_rdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwfWMnqPJYdM"
      },
      "source": [
        "`groupBy(func)` returns a RDD with its data grouped in a key/value format, using a function to obtain the key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2_KQTlLJcuz"
      },
      "outputs": [],
      "source": [
        "grouped_rdd = distinct_rdd.groupBy(lambda x: x%3) #remainder of dividing by 3\n",
        "print(grouped_rdd.collect())\n",
        "print([(x,sorted(y)) for (x,y) in grouped_rdd.collect()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_aqy32_J5qb"
      },
      "source": [
        "### Transformations on two RDDs\n",
        "\n",
        "Set operations on two RDDs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV0qWikBJ-M1"
      },
      "source": [
        "-   `rdda.union(rddb)` returns a RDD with all the elements from the input RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii9CxKQnKAfG"
      },
      "outputs": [],
      "source": [
        "rdda = sc.parallelize(['a', 'b', 'c'])\n",
        "rddb = sc.parallelize(['c', 'd', 'e'])\n",
        "rddu = rdda.union(rddb)\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_rddu(self):\n",
        "        self.assertEqual(rddu.collect(),['a', 'b', 'c', 'c', 'd', 'e'])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlgU-Ke9KDbR"
      },
      "source": [
        "- `rdda.intersection(rddb)` returns a new RDD with the elements in common in both RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TyzvnyHKF3E"
      },
      "outputs": [],
      "source": [
        "rddi = rdda.intersection(rddb)\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_rddi(self):\n",
        "        self.assertEqual(rddi.collect(),['c'])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2ISYjhOKNQn"
      },
      "source": [
        "-   `rdda.subtract(rddb)` returns a new RDD with the elements from the second RDD substracted to those from the first RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvI0JeqkKQBR"
      },
      "outputs": [],
      "source": [
        "rdds = rdda.subtract(rddb)\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_rdds(self):\n",
        "        self.assertEqual(rdds.collect(), ['b', 'a'])\n",
        "\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHOo_LyUKTAP"
      },
      "source": [
        "- `rdda.cartesian(rddb)` cartesian product of both RDDs (heavy operation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrMKdf03KW9-"
      },
      "outputs": [],
      "source": [
        "rddc = rdda.cartesian(rddb)\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_rddc(self):\n",
        "        self.assertEqual(rddc.collect(),[('a','c'),('a','d'),('a','e'),('b','c'),('c','c'),('b','d'),('b','e'), ('c','d'), ('c','e')])\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uzixrgv33i-"
      },
      "source": [
        "###Actions on simple RDDs\n",
        "\n",
        "\n",
        "Obtain (simple or complex) elements from a RDD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WobYnSRy4Waq"
      },
      "source": [
        "#### Main aggregate actions: `reduce` and `fold`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwsIdw64hu8"
      },
      "source": [
        "-   `reduce(op)` combines elements from a RDD in parallel, using an operator\n",
        "    - The reduction operator must be an associative-and-commutative binary operator\n",
        "    - The reduction is first computed at partition level and then the intermediate values are reduced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U5Z3LCZ4hA8"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize(range(1,10), 8)  # range [1, 10)\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "# Reduction using a lambda function\n",
        "p = rdd.reduce(lambda x,y: x*y) # r = 1*2*3*4*5*6*7*8*9 = 362880\n",
        "print(\"1*2*3*4*5*6*7*8*9 = {0}\".format(p))\n",
        "\n",
        "# Reduction using a predefined operator\n",
        "from operator import add\n",
        "s = rdd.reduce(add) # s = 1+2+3+4+5+6+7+8+9 = 45\n",
        "print(\"1+2+3+4+5+6+7+8+9 = {0}\".format(s))\n",
        "\n",
        "# Test with a non-commutative operator\n",
        "p = rdd.reduce(lambda x,y: x-y) # r = 1-2-3-4-5-6-7-8-9 = -43\n",
        "print(\"1-2-3-4-5-6-7-8-9 = {0}\".format(p))\n",
        "\n",
        "# It does not work with empty RDDs\n",
        "#sc.parallelize([]).reduce(add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fJ8RRbX4rYl"
      },
      "source": [
        "-   `fold(zero, op)` general version of `reduce`:\n",
        "    - An initial zero value must be provided to the operator\n",
        "    - The initial value must be the operator's identity value (i.e. 0 for addition; 1 for product, or an empty list for a list concatenation)\n",
        "        - It works with empty RDDs\n",
        "    - The `op` function must be a commutative monoid to ensure a consistent output\n",
        "        - Different behaviour compared to `fold` operations of languages such as Scala\n",
        "        - The operator is applied at partition level (using `zero` as initial value), and late between all partitions (using `zero` again)\n",
        "        - For non-commutative operators, the output might be different from the one obtained using a sequential `fold`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sPNN_Tj4ujN"
      },
      "outputs": [],
      "source": [
        "!python -V\n",
        "\n",
        "#addition\n",
        "rdd1 = sc.parallelize([1,3,2,4,2], 2)\n",
        "print(rdd1.glom().collect())\n",
        "f1 = rdd1.fold(0,lambda a,b:a+b )\n",
        "print(f1)\n",
        "\n",
        "\n",
        "#product\n",
        "rdd2 = sc.parallelize([1,3,2,4,2], 3)\n",
        "print(rdd2.glom().collect())\n",
        "f2 = rdd2.fold(1,lambda a,b:a*b )\n",
        "print(f2)\n",
        "\n",
        "#max\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "data = sc.parallelize([('a', 1), ('b', 2),('c', 0), ('d', 3)])\n",
        "zero = (None, float(\"-Inf\"))\n",
        "op = partial(max, key=itemgetter(1))\n",
        "\n",
        "f3 = data.fold(zero,op)\n",
        "print(f3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9t5XLca43dO"
      },
      "source": [
        "#### Other aggregate functions: `aggregate`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khJ7RlMC49pE"
      },
      "source": [
        " - `aggregate(zero,seqOp,combOp)`: Returns a collection by aggregating the RDD elements using two functions:\n",
        "      1. `seqOp` -  partition-level aggregation: an accumulator is created by partition (initialised to `zero`); values from the partition are aggregated in the accumulator\n",
        "      2. `combOp` - Aggregation between partitions: all accumulators from each partition are aggregated\n",
        "      -  Both aggregations use an initial value `zero` (similar to the `fold` case).\n",
        "\n",
        "- General version of `reduce` and `fold`    \n",
        "- The first function (`seqOp`) can return a type, U, different to the T type of the RDD elements\n",
        "    - `seqOp` aggregates T-type elements and returns a U-type\n",
        "    - `combOp` aggregates U-type elements\n",
        "    - `zero` must be of U-type\n",
        "\n",
        "- Can return a type different from the input RDD elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtqMFe975iTf"
      },
      "outputs": [],
      "source": [
        "# Example: compute the average of an RDD\n",
        "#            (0, 0) <-- zeroValue\n",
        "#\n",
        "#    [1, 2]              [3, 4]\n",
        "#\n",
        "#    0 + 1 = 1           0 + 3 = 3  -> seqOp\n",
        "#    0 + 1 = 1           0 + 1 = 1\n",
        "#\n",
        "#    1 + 2 = 3           3 + 4 = 7\n",
        "#    1 + 1 = 2           1 + 1 = 2\n",
        "#        |                   |\n",
        "#        v                   v\n",
        "#     (3, 2)              (7, 2)\n",
        "#         \\                 /\n",
        "#          \\               /\n",
        "#           \\             /\n",
        "#            \\           /\n",
        "#             ------------\n",
        "#             |  combOp  |\n",
        "#             ------------\n",
        "#                  |\n",
        "#                  v\n",
        "#               (10, 4)\n",
        "\n",
        "seqOp = (lambda local_result, list_element: (local_result[0] + list_element, local_result[1] + 1) )\n",
        "rddSeqOp = rdd.filter(seqOp)\n",
        "combOp = (lambda some_local_result, another_local_result: (some_local_result[0] + another_local_result[0], some_local_result[1] + another_local_result[1]) )\n",
        "sumCount = sc.parallelize([1, 2, 3, 4],2).aggregate((0, 0), seqOp, combOp)\n",
        "print(sumCount)\n",
        "sumCount[0] / float(sumCount[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcRe74tg5nwR"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "l = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "rdd = sc.parallelize(l)\n",
        "\n",
        "# acc is a three-element tuple (List, Double, Int)\n",
        "# The squared elements of RDD are concatenated in the first element of acc (list).\n",
        "# The second element accumulates the RDD elements after a multiplication\n",
        "# The third one stores the number of elements of the RDD\n",
        "seqOp  = (lambda acc, val: (acc[0]+[val*val],\n",
        "                            acc[1]*val,\n",
        "                            acc[2]+1))\n",
        "# For each partition, a new tuple of type acc is created\n",
        "# In this operation, the three elements of the tuples are combined\n",
        "combOp = (lambda acc1, acc2: (acc1[0]+acc2[0],\n",
        "                              acc1[1]*acc2[1],\n",
        "                              acc1[2]+acc2[2]))\n",
        "\n",
        "a = rdd.aggregate(([], 1., 0), seqOp, combOp)\n",
        "\n",
        "print(a)\n",
        "\n",
        "\n",
        "class TestNotebook(unittest.TestCase):\n",
        "\n",
        "    def test_a(self):\n",
        "\n",
        "        self.assertEqual(a[1],8.*7.*6.*5.*4.*3.*2.*1.)\n",
        "        self.assertEqual(a[2], len(l))\n",
        "\n",
        "unittest.main(argv=[''], verbosity=2, exit=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R7X1Mqt5xXV"
      },
      "source": [
        "#### Actions to count elements on RDDs\n",
        "- `count()` returns an integer with the exact number of elements in the RDD\n",
        "- `countApprox(timeout, confidence=0.95)` approximate version of `count()`, it returns a result potentially incomplete in a maximal time, even if not all tasks have finished (Experimental).\n",
        "    - `timeout` is a long integer and gives the time in milliseconds\n",
        "    - `confidence` probability of obtaining the the actual value. If `confidence` is 0.90 it means that, after several executions, we expect that 90% of them return the right value. Output value in [0,1]\n",
        "- `countApproxDistinct(relativeSD=0.05)` returns an estimation of the number of different elements in the RDD (Experimental).\n",
        "    - `relativeSD` – relative accuracy (smaller values imply a lower error, but require more memory; they must be higher than 0.000017).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2RtxZIv6F4U"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([i % 20 for i in range(10000)], 16)\n",
        "print(\"Total number of elements: {0}\".format(rdd.count()))\n",
        "print(\"Number of different elements: {0}\".format(rdd.distinct().count()))\n",
        "\n",
        "print(\"Total number of elements (approx.): {0}\".format(rdd.countApprox(1, 0.4)))\n",
        "print(\"Number of different elements (approx.): {0}\".format(rdd.countApproxDistinct(0.5)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peDFQ_z36KBW"
      },
      "source": [
        "-   `countByValue()` returns the number of appearances of each element in the RDD as a map (or dictionary) in a key/value fashion\n",
        "    - Keys are the RDD elements. Values are the number of occurrences of the associated key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KGIFUB7t6Ry0"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize(list(\"abracadabra\")).cache()\n",
        "mymap = rdd.countByValue()\n",
        "\n",
        "print(type(mymap))\n",
        "print(mymap.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM_x4BLA6War"
      },
      "source": [
        "#### Actions to obtain values\n",
        "- These methods must be used with care: if the expected result is very large the driver memory can be saturated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3MQ5lPi6ZsV"
      },
      "source": [
        "-   `collect()` returns a list with all the RDD elements\n",
        "-   `show()` shows the elements as a table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DF5OD7c6SN_"
      },
      "outputs": [],
      "source": [
        "mylist = rdd.collect()\n",
        "print(mylist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDXLHPRk6fqk"
      },
      "source": [
        "-   `take(n)` returns the `n` first elements of the RDD\n",
        "-   `takeSample(withRep, n, [seed])` returns `n` random elements of the RDD\n",
        "    - `withRep`: if True, the same element can appear several times in the sample\n",
        "    - `seed`: seed for the random number generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZeRxNci6lKA"
      },
      "outputs": [],
      "source": [
        "t = rdd.take(4)\n",
        "print(t)\n",
        "s = rdd.takeSample(False, 4)\n",
        "print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VDC33Oo6rfq"
      },
      "source": [
        "-   `top(n)` returns a list with the first `n` elements of the RDD sorted in descending order\n",
        "-   `takeOrdered(n,[order])` returns a list with the first `n` elements of the RDD in ascending order (as opposite to `top`), or following the order indicated as a function in the optional parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-MjWWQo6tc9"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([8, 4, 2, 9, 3, 1, 10, 5, 6, 7]).cache()\n",
        "\n",
        "print(\"4 largest elements: {0}\".format(rdd.top(4)))\n",
        "\n",
        "print(\"4 smallest elements: {0}\".format(rdd.takeOrdered(4)))\n",
        "\n",
        "print(\"4 largest elements: {0}\".format(rdd.takeOrdered(4, lambda x: -x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyAADA2J_ncL"
      },
      "source": [
        "## RDDs of key/value pairs (aka *Pair RDDs*)\n",
        "\n",
        "-   Data types heavily used on Big Data (MapReduce)\n",
        "\n",
        "-   Spark provides special operation to handle them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVyfXBFZAWVV"
      },
      "source": [
        "### Creation of *Pair RDDs*\n",
        "Key/value RDDs can be created from a list of tuples, from another RDD or from a zip of two RDDs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfw7YLtkAc8z"
      },
      "source": [
        "-   From a list of tuples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB0ZdRCIA5GU"
      },
      "outputs": [],
      "source": [
        "prdd = sc.parallelize([('a',2), ('b',5), ('a',3)])\n",
        "print(prdd.collect())\n",
        "\n",
        "prdd = sc.parallelize(zip(['a', 'b', 'c'], range(3)))\n",
        "print(prdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSijjQ9MBvAz"
      },
      "source": [
        "-   From another RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb-s7RgoBwX6"
      },
      "outputs": [],
      "source": [
        "# Example using a file\n",
        "# For each line we get a tuple, the first element being the first word of the line\n",
        "# and the second one the whole line\n",
        "linesrdd = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"/quijote.txt\", use_unicode=True)\n",
        "prdd = linesrdd.map(lambda x: (x.split(\" \")[0], x))\n",
        "\n",
        "print('Pair (1st word, line): {0}\\n'.format(prdd.takeSample(False, 3)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSoBpRnCB7ij"
      },
      "outputs": [],
      "source": [
        "# Using keyBy(f): Creates tuples of the RDD elements using f to obtain the key.\n",
        "nrdd = sc.parallelize(range(2,5))\n",
        "prdd = nrdd.keyBy(lambda x: x*x)\n",
        "\n",
        "print(prdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEGXu3EFB9xj"
      },
      "outputs": [],
      "source": [
        "# zipWithIndex(): Zips the RDD with its elements' indexes.\n",
        "rdd = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
        "prdd = rdd.zipWithIndex()\n",
        "print(rdd.glom().collect())\n",
        "\n",
        "print(prdd.collect())\n",
        "\n",
        "# This method starts a Spark job when the RDD has more than one partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mu5JfQX1CGRt"
      },
      "outputs": [],
      "source": [
        "# zipWithUniqueId(): Zips the RDD with unique identifiers (long) for each element.\n",
        "# Elements in the k-th partition get the k, n+k, 2*n+k,... ids, being n = number of partitions\n",
        "# It does not start a Spark job\n",
        "rdd = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'], 3)\n",
        "print(\"RDD partitions: {0}\".format(rdd.glom().collect()))\n",
        "prdd = rdd.zipWithUniqueId()\n",
        "\n",
        "print(prdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQpK7Al4B4u_"
      },
      "source": [
        "- From a zip of two RDDs\n",
        "    - RDDs must have the same number of partitions and the same number of elements on each partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AA5lHTHBR6bn"
      },
      "outputs": [],
      "source": [
        "rdd1 = sc.parallelize(range(0, 5), 2)\n",
        "\n",
        "rdd2 = sc.parallelize(range(1000, 1005), 2)\n",
        "\n",
        "prdd = rdd1.zip(rdd2)\n",
        "\n",
        "print(prdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUcY18f3CYcW"
      },
      "source": [
        "### Transformations on a single RDD containing key/value pairs\n",
        "On a single RDD key/value we can perform key-based aggregation transformations as well as transformations affecting keys or values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9CeV-c4CfnW"
      },
      "source": [
        "#### Aggregation transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkbKWP9CCs8J"
      },
      "source": [
        "-   `reduceByKey(func)`/`foldByKey(func)`\n",
        "    -  Return a RDD, grouping all values associated to the same key according to the `func` function\n",
        "    -  Similar to `reduce` and `fold` on simple RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QPGdWf6DYrl"
      },
      "outputs": [],
      "source": [
        "from operator import add\n",
        "prdd   = sc.parallelize([('a', 2), ('b', 5), ('a', 8), ('b', 6), ('b', 2)]).cache()\n",
        "redrdd = prdd.reduceByKey(add)\n",
        "\n",
        "print(redrdd.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BX9l_3aDbiJ"
      },
      "source": [
        "-   `groupByKey()` groups values associated to the same key\n",
        "    - Operation very costly on communications\n",
        "    - It is usually better to use a reduction operation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RbKXqYyDiR2"
      },
      "outputs": [],
      "source": [
        "grouprdd = prdd.groupByKey()\n",
        "\n",
        "print(grouprdd.collect())\n",
        "print\n",
        "\n",
        "mylist = [(k, list(v)) for k, v in grouprdd.collect()]\n",
        "print(mylist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXsEGHEsDpZn"
      },
      "source": [
        "- `combineByKey(createCombiner(func1), mergeValue(func2), mergeCombiners(func3))`\n",
        "    - General method for key-based aggregation, similar to `aggregate`\n",
        "    - It requires three functions:\n",
        "\n",
        "     1.  `createCombiner` when going through the elements of each partition, if we find a new key, a new cummulator is created and initialised with `func1`\n",
        "\n",
        "     2.  `mergeValue` merges values of each key in each partition using `func2`\n",
        "\n",
        "     3.  `mergeCombiners` merges the results from the different partitions using `func3`\n",
        "\n",
        "- The values of the output RDD can have a type different from the values of the input RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV8DjwlBDuqJ"
      },
      "outputs": [],
      "source": [
        "# For each key, it obtains a tuple with the addition and the number of values\n",
        "sumCount = prdd.combineByKey(\n",
        "                            (lambda x: (x, 1)),\n",
        "                            (lambda x, y: (x[0]+y, x[1]+1)),\n",
        "                            (lambda x, y: (x[0]+y[0], x[1]+y[1])))\n",
        "\n",
        "print(sumCount.collect())\n",
        "\n",
        "# With this RDD, we obtain the average value for each key\n",
        "m = sumCount.mapValues(lambda v: float(v[0])/v[1])\n",
        "print(m.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwIinMjuGTir"
      },
      "source": [
        "#### Transformations on keys or values\n",
        "-   `keys()` returns a RDD with the keys\n",
        "-   `values()` returns a RDD with the values\n",
        "-   `sortByKey()` returns a RDD key/value with the keys sorted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RprhvEhGXUy"
      },
      "outputs": [],
      "source": [
        "print(\"RDD complete: {0}\".format(prdd.collect()))\n",
        "print(\"RDD with the keys: {0}\".format(prdd.keys().collect()))\n",
        "print(\"RDD with the values: {0}\".format(prdd.values().collect()))\n",
        "print(\"RDD the sorted keys: {0}\".format(prdd.sortByKey().collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DI8fVvJGjJZ"
      },
      "source": [
        "-   `mapValues(func)` returns a RDD applying a function on its values\n",
        "-   `flatMapValues(func)` returns a RDD applying a function on its values and \"flattening\" the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmnXE5pyGmK4"
      },
      "outputs": [],
      "source": [
        "mapv = prdd.mapValues(lambda x: (x, 10*x))\n",
        "print(mapv.collect())\n",
        "\n",
        "fmapv = prdd.flatMapValues(lambda x: (x, 10*x))\n",
        "print(fmapv.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzGVWWLYGreo"
      },
      "source": [
        "### Transformations on two RDDs containing key/value pairs\n",
        "Combine two RDDs of type key/value to obtain a third RDD."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZiT6m0wGw1N"
      },
      "source": [
        "- `join`/`leftOuterJoin`/`rightOuterJoin`/`fullOuterJoin` perform inner/outer/full joins on the two RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxz5sB5oG2TH"
      },
      "outputs": [],
      "source": [
        "rdd1 = sc.parallelize([(\"a\", 2), (\"b\", 5), (\"a\", 8)]).cache()\n",
        "rdd2 = sc.parallelize([(\"c\", 7), (\"a\", 1)]).cache()\n",
        "\n",
        "rdd3 = rdd1.join(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FOea7fAG44n"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.leftOuterJoin(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imBLvb0PG8X3"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.rightOuterJoin(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSnOVgqNG94b"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.fullOuterJoin(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbR8z_LTHCcd"
      },
      "source": [
        "-   `subtractByKey` deletes elements with a key present in another RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuTlI8iEHHBQ"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.subtractByKey(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_VWBLneHLyq"
      },
      "source": [
        "-   `cogroup` groups the elements that share the same key in both RDDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNMwjFgsHPmW"
      },
      "outputs": [],
      "source": [
        "rdd3 = rdd1.cogroup(rdd2)\n",
        "\n",
        "print(rdd3.collect())\n",
        "\n",
        "map = rdd3.mapValues(lambda v: [list(l) for l in v]).collectAsMap()\n",
        "\n",
        "print(map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMFwM1PHZx7"
      },
      "source": [
        "### Actions on RDDs key/value\n",
        "On RDDs key/value we can apply the same actions as for simple RDDs, as well as some additional ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5SRPugkHdRj"
      },
      "source": [
        "-   `collectAsMap()` obtains a RDD as a map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a6uJnHGHif_"
      },
      "outputs": [],
      "source": [
        "prdd = sc.parallelize([(\"a\", 7), (\"b\", 5), (\"a\", 8)]).cache()\n",
        "\n",
        "rddMap = prdd.collectAsMap()\n",
        "\n",
        "print(rddMap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA7kaVM2HgjA"
      },
      "source": [
        "-   `countByKey()` returns a map with the number of occurrences of each key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xavrSmxKHmxh"
      },
      "outputs": [],
      "source": [
        "countMap = prdd.countByKey()\n",
        "\n",
        "print(countMap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLA2qNycHp6N"
      },
      "source": [
        "-   `lookup(key)` returns a list with the values associated to a given key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sFDk3YzHsTY"
      },
      "outputs": [],
      "source": [
        "listA = prdd.lookup('a')\n",
        "\n",
        "print(listA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts0Zsb4fKFdd"
      },
      "source": [
        "##Numeric RDDs\n",
        "\n",
        "\n",
        "Descriptive statistics operations provided by Spark\n",
        "\n",
        "  Method              |  Description                       \n",
        "  ------------------- | ----------------------------------\n",
        "  stats()             | Statistics overview            \n",
        "  mean()              | Average of the elements\n",
        "  sum(), max(), min() | Total, maximum value and minimum value\n",
        "  variance()          | Variance of the elements\n",
        "  sampleVariance()    | Variance of the elements, computed for a sample\n",
        "  stdev()             | Standard deviation\n",
        "  sampleStdev()       | Sample standard deviation\n",
        "  histogram()         | Histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSq_Xio1KPf7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Consider an RDD of random data with normal distribution\n",
        "nrdd = sc.parallelize(np.random.normal(size=10000)).cache()\n",
        "\n",
        "# Statistics overview\n",
        "sts = nrdd.stats()\n",
        "\n",
        "print(\"Statistics overview:\\n {0}\\n\".format(sts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlYGN9ffKVJK"
      },
      "outputs": [],
      "source": [
        "from math import fabs\n",
        "\n",
        "# Filter outliers\n",
        "stddev = sts.stdev()\n",
        "avg = sts.mean()\n",
        "\n",
        "frdd = nrdd.filter(lambda x: fabs(x - avg) < 3*stddev).cache()\n",
        "\n",
        "print(\"Number of outliers: {0}\".format(sts.count() - frdd.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJJc_21GKbKw"
      },
      "outputs": [],
      "source": [
        "#import base64\n",
        "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "def show(p):\n",
        "    img = StringIO.StringIO()\n",
        "    p.savefig(img, format='svg')\n",
        "    img.seek(0)\n",
        "    print (\"%html <div style='width:600px'>\" + img.buf + \"</div>\")\n",
        "\n",
        "# Get a histogram with 10 groups\n",
        "x,y = frdd.histogram(10)\n",
        "\n",
        "# Clean the plot\n",
        "plt.gcf().clear()\n",
        "plt.figure()\n",
        "plt.bar(x[:-1], y, width=0.6)\n",
        "plt.xlabel(u'Values')\n",
        "plt.ylabel(u'Number of occurrences')\n",
        "plt.title(u'Histogram')\n",
        "\n",
        "#show(plt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhRQQgPWM9ZU"
      },
      "source": [
        "## Reading and writing RDDs from/to files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-krfCkdNDt2"
      },
      "source": [
        "### Supported filesystems\n",
        "-   Like Hadoop, Spark supports different filesystems: local, HDFS, Amazon S3\n",
        "\n",
        "    -   In general, it supports any data source that can be read with Hadoop\n",
        "\n",
        "-   It can access relational or noSQL databases\n",
        "\n",
        "    -   MySQL, Postgres, etc. using JDBC\n",
        "    -   Apache Hive, HBase, Cassandra or Elasticsearch\n",
        "\n",
        "### Supported file formats\n",
        "\n",
        "-   Spark can access different file types:\n",
        "\n",
        "    -   Plain text, CSV, sequence files, JSON, *protocol buffers* and *object files*\n",
        "        -   It supports compressed files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlI-91DzNekY"
      },
      "source": [
        "### Examples with text files\n",
        "\n",
        "In the `../yourDrive/books` directory there is a collection of compressed text files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqI6MeqkNirc"
      },
      "outputs": [],
      "source": [
        "# Input files\n",
        "!ls \"$DRIVE_DATA\"/books"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbrWdpANn69"
      },
      "source": [
        "### Functions for reading and writing text files\n",
        "\n",
        "\n",
        "- `sc.textFile(file/directory_name)` It creates an RDD from the lines of one or several text files\n",
        "    - If a directory is specified, all files inside will be read, creating a partition per file\n",
        "    - Files can be compressed, also in different formats (gz, bz2,...)\n",
        "    - Wildcards can be specified in the file name\n",
        "- `sc.wholeTextFiles(file/directory_name)` Reads the file and returns a key/value RDD\n",
        "    - key: File absolute path\n",
        "    - value: The whole text of the file\n",
        "- `rdd.saveAsTextFile(output_directory)` Stores the RDD in text format in the specified directory\n",
        "    - Creates a file for each partition of rdd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEC3rkJwOGl-"
      },
      "outputs": [],
      "source": [
        "# Reads all files in the directory\n",
        "# and creates an RDD with their lines\n",
        "lines = sc.textFile(os.environ[\"DRIVE_DATA\"] + \"books\")\n",
        "\n",
        "# Creates a partition per input file\n",
        "print(\"Number of partitions of the RDD lines = {0}\".format(\n",
        "       lines.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q7IIuqxOrpI"
      },
      "outputs": [],
      "source": [
        "# Gets the words using the split method (split uses a space as default delimiter)\n",
        "words = lines.flatMap(lambda x: x.split())\n",
        "print(\"Number of partitions of the RDD words = {0}\".format(\n",
        "       words.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjpgVQpXOss6"
      },
      "outputs": [],
      "source": [
        "# Repartitioning the RDD in 4 partitions\n",
        "words2 = words.coalesce(4)\n",
        "print(\"Number of partitions of the RDD words2 = {0}\".format(\n",
        "       words2.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLGz4yHfPc1R"
      },
      "outputs": [],
      "source": [
        "# Takes a random sample from the words\n",
        "print(words2.takeSample(False, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrZjjtogPgtU"
      },
      "outputs": [],
      "source": [
        "# Saves the RDD words as several output files\n",
        "# (a file per partition)\n",
        "words2.saveAsTextFile(\"file:///tmp/outputtxt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR7xk_6xPkh8"
      },
      "outputs": [],
      "source": [
        "# output files\n",
        "!ls -l /tmp/outputtxt\n",
        "!head /tmp/outputtxt/part-00002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pov4YA9PpKA"
      },
      "outputs": [],
      "source": [
        "# Reads the files and returns a key/value RDD\n",
        "# key->file name, value->complete file\n",
        "prdd = sc.wholeTextFiles(os.environ[\"DRIVE_DATA\"] +\"books/p*.gz\")\n",
        "print(\"Number of partitions of the RDD prdd = {0}\\n\".format(\n",
        "       prdd.getNumPartitions()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7P6tJ1pQiJ2"
      },
      "outputs": [],
      "source": [
        "# Gets a key/value list\n",
        "# key->file name, value->number of words\n",
        "mylist = prdd.mapValues(lambda x: len(x.split())).collect()\n",
        "\n",
        "for book in mylist:\n",
        "    print(\"The book {0:14s} has {1:6d} words\".format(\n",
        "           book[0].split(\"/\")[-1], book[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32YS9pB0Qtbq"
      },
      "source": [
        "### Sequence files\n",
        "Flat files used in Hadoop consisting of binary key/value pairs\n",
        "\n",
        "-   Their elements implement the [`Writable`](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html) interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTjyykBEQzRc"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize([(\"a\",2), (\"b\",5), (\"a\",8)], 2)\n",
        "\n",
        "# We save the key/value RDD as a Sequence file\n",
        "rdd.saveAsSequenceFile(\"file:///tmp/sequenceoutdir2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyhKT_eQ-lN"
      },
      "outputs": [],
      "source": [
        "!echo 'output directory'\n",
        "!ls -l /tmp/sequenceoutdir2\n",
        "#!echo 'Attempt to read one of the files'\n",
        "#!cat /tmp/sequenceoutdir2/part-00000\n",
        "#!echo\n",
        "!echo  'Read the file using Hadoop'\n",
        "!wget https://archive.apache.org/dist/hadoop/core/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n",
        "!tar xf hadoop-2.7.3.tar.gz\n",
        "!rm hadoop-2.7.3.tar.gz\n",
        "!/content/hadoop-2.7.3/bin/hdfs dfs -text /tmp/sequenceoutdir2/part-00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndlvU2czRGxj"
      },
      "outputs": [],
      "source": [
        "# We read the file in another RDD\n",
        "rdd2 = sc.sequenceFile(\"file:///tmp/sequenceoutdir2\",\n",
        "                       \"org.apache.hadoop.io.Text\",\n",
        "                       \"org.apache.hadoop.io.IntWritable\")\n",
        "\n",
        "print(\"Content of the RDD {0}\".format(rdd2.collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjEP3YwEV-7W"
      },
      "source": [
        "### Hadoop input/output formats\n",
        "Spark can interact with any file format supported by Hadoop\n",
        "- It supports the \"old\" and \"new\" APIs\n",
        "- It supports accessing other storage types (not files), e.g. HBase or MongoDB, using `saveAsHadoopDataSet` and/or `saveAsNewAPIHadoopDataSet`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pltQUkchWBFx"
      },
      "outputs": [],
      "source": [
        "# We save the key/value RDD as a Hadoop text file (TextOutputFormat)\n",
        "rdd.saveAsNewAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\",\n",
        "                            \"org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\",\n",
        "                            \"org.apache.hadoop.io.Text\",\n",
        "                            \"org.apache.hadoop.io.IntWritable\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CokGimr9WFDD"
      },
      "outputs": [],
      "source": [
        "!echo 'Output directory'\n",
        "!ls -l /tmp/hadoopfileoutdir\n",
        "!cat /tmp/hadoopfileoutdir/part-r-00001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7wpP5SZWK9y"
      },
      "outputs": [],
      "source": [
        "# We read it as a Hadoop key/value file (KeyValueTextInputFormat)\n",
        "rdd3 = sc.newAPIHadoopFile(\"file:///tmp/hadoopfileoutdir\",\n",
        "                          \"org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\",\n",
        "                          \"org.apache.hadoop.io.Text\",\n",
        "                          \"org.apache.hadoop.io.IntWritable\")\n",
        "\n",
        "print(\"Content of the RDD {0}\".format(rdd3.collect()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O3IAwByxTpJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ708zvl627q"
      },
      "source": [
        "### Exercise 5.1: Word count\n",
        "\n",
        "**Using RDDs**, count the number of lines in the `$DRIVE_DATA/quijote.txt` file. Then, count the number of words in the file. Finally, count the number of *different* words in the file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slr4nXhp_knl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjtVkYOKxTo7"
      },
      "source": [
        "### Exercise 5.2: Count people by age\n",
        "\n",
        "Using RDDs, create a barplot showing of number of people (y-axis) per age (x-axis) using the information in the $DRIVE_DATA/people.txt file\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JgE0XavBBLB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xddDZ2zCxTor"
      },
      "source": [
        "### Exercise 5.3: Obtain the number of received citations\n",
        "\n",
        "Using RDDs, write a PySpark program that obtains, from the cite75_99.txt file, the number of citations received by each patent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmYIlZ0dCQ6x"
      },
      "outputs": [],
      "source": [
        "!ls \"$DRIVE_DATA\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}